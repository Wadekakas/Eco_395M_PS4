---
title: "Data mining Homework 4"
author: "Ziyue Wang ChenYen Liu Yuzhu Liu"
date: "4/8/2023"
output: pdf_document
---

## Question 1
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

options(repos = c(CRAN = "http://cran.rstudio.com"))
install.packages("fields")
install.packages("locfit")
library(ggplot2)
library(tidyverse)
library(knitr)
library(reshape2)
library(foreach)
library(gridExtra)
library(mosaic)
library(LICORS)

```


##Question 1: Clustering and PCA


### Clustering using K-means++
we use K mean++ initialization to separate the data into 2 different clusters since we are trying to find if the unsupervised algorithm can distinguish to reds from the whites just by looking at the 11 chemicals.


```{r clustering, echo=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
### data cleaning 
# Center and scale the data
X = wine[ ,-(12:13)]
X = scale(X, center=TRUE, scale=TRUE)
# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
# Using kmeans++ initialization
clust_wine = kmeanspp(X, k=2, nstart=25)
```

#### First try: using `residual.sugar` and `alcohol`

To begin with, we selected residual.sugar and alcohol variables at random to serve as the plot's axes and identify cluster membership. However, the resulting plot did not effectively display the cluster membership using only these two variables.

```{r bad cluster, echo=FALSE}
# A few plots with cluster membership shown
# Try some variables not are not gonna look great 
ggplot(wine) + 
  geom_point(aes(residual.sugar, alcohol, color = factor(clust_wine$cluster), shape=factor(clust_wine$cluster)))+
    labs(x ="Residual Sugar", y ="Alcohol ", title = "Cluster Membership")+
          theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```



Unsurprisingly, this way of view the clusters is not good for us to distinguish White from Red as we can see from the plot below:

```{r bad_cluster, echo=FALSE}
ggplot(wine) + 
  geom_point(aes(residual.sugar, alcohol, color = color , shape=factor(clust_wine$cluster)))+   scale_color_manual(values=c("red", "grey"))+ guides(shape =guide_legend(title="Cluster"))+
    labs(x ="Residual Sugar", y ="Alcohol ", title = " Is the Clusters Separating White from Red?")+
          theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```
#### Second try: using `volatile.acidity` and `total.sulfur.dioxide`

Now, we try choosing two other variables `volatile.acidity` and `total.sulfur.dioxide` to see the membership.
```{r good_cluster, echo=FALSE}
# Hand picked 2 variables to make it look good 
# First see how does our clusters look like on a "" v.s "" map
ggplot(wine) + 
  geom_point(aes(volatile.acidity, total.sulfur.dioxide, color = factor(clust_wine$cluster), shape=factor(clust_wine$cluster)))+
    labs(x ="Volatile Acidity ", y ="Total Sulfur Dioxide ", title = "Cluster Membership")+
          theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```


Now again, match the clusters with red/white, we can see this way of viewing the cluster is good for us to distinguish whites from reds

```{r, echo=FALSE}
## NOW, match the clusters with red/white
ggplot(wine) + 
  geom_point(aes(volatile.acidity, total.sulfur.dioxide, color = color , shape=factor(clust_wine$cluster)))+ scale_color_manual(values=c("red", "grey"))+ guides(shape =guide_legend(title="Cluster"))+
    labs(x ="Volatile Acidity", y ="Total Sulfur Dioxide", title = " Is the Clusters Separating White from Red?")+
          theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```


Although this is good at distinguishing whites from reds, it is not too great at distinguishing the quality of the wines.

```{r, echo=FALSE}
# not too great at seperating the quality
ggplot(wine) + 
  geom_point(aes(volatile.acidity, total.sulfur.dioxide, color = quality, shape=factor(clust_wine$cluster)))+ 
  guides(shape =guide_legend(title="Cluster"))+
    labs( title = "Quality of the Wine?")+
          theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```
### PCA

Now, we try running PCA on the data 
```{r PCA, echo=FALSE}
# Now run PCA on the  data
pc_wine = prcomp(X, rank=5, scale=TRUE)
loadings = pc_wine$rotation
scores = pc_wine$x
```


We can briefly examine the linear combinations of data that define the principal components (PCs), where each column represents a distinct linear summary of the 11 chemicals.

```{r PCA -2, echo=FALSE}
# these are the linear combinations of data that define the PCs
# each column is a different linear summary of the 11 chemicals 
kable(head(loadings))
```



By utilizing five summary features, we can capture 80% of the total variation observed in the 11 original features. Although the compression ratio may not seem impressive, it is adequate to differentiate between red and white wines.

```{r PCA -3, echo=FALSE}
# 5 summary features gets us 80% of the overall variation in the 11 original features
# although the compression ratio does not look great, it is sufficient to distinguish reds and whites 
summary(pc_wine)
qplot(scores[,1], scores[,2], color=wine$color, xlab='Component 1', ylab='Component 2') + scale_color_manual(values=c("red", "grey"))
```


But it is still very hard to tell the quality of the wine from PCs.



```{r PCA -4, echo=FALSE}
# The PCA separates the color of the wine well
# but not the quality of the wine
qplot(scores[,1], scores[,3], color=wine$quality, xlab='Component 1', ylab='Component 3')
```

### Conclusion
Employing PCA could prove more effective in differentiating between red and white wines, as it eliminates the need to select specific variables to construct the map displaying the clusters. Rather, PCA enables us to use the two principal components to distinguish between the two types of wines.


It is worth noting, however, that neither of these unsupervised learning methods can accurately distinguish between higher and lower quality wines.

## Question 2
```{r, echo=FALSE, warning=FALSE, message=FALSE,fig.width=18, fig.height=13}
knitr::opts_chunk$set(echo = TRUE)
 library(tidyverse)
library(ClusterR)  # for kmeans++
library(foreach)
library(mosaic)
library(ggplot2)
library(ggcorrplot)
library(foreach)
library(gridExtra)
social_marketing = read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv', header=TRUE)

X = social_marketing[,-(1)]
X = scale(X, center=TRUE, scale=TRUE)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")

ggcorrplot::ggcorrplot(cor(X), hc.order = TRUE)
```

By generating a brief correlation plot, we can identify the tweet categories that exhibit the strongest association with one another for a particular user. 

Our next step involves employing the K-means algorithm to cluster the user's Twitter followers based on their frequency of tweets in specific categories, which may help us uncover interesting subgroups. However, before proceeding, we must determine the most appropriate number of clusters to use, given that the tweets are divided into numerous variables.

```{r echo=FALSE, message=FALSE, warning=FALSE}

##find optimal K
k_grid = seq(2, 30, by=1) 
SSE_grid = foreach(k = k_grid, .combine='c') %do% {
cluster_k = kmeans(X, k, nstart=25)
cluster_k$tot.withinss } 

plot(k_grid, SSE_grid, main = "Elbow Plot")
```
According to the graph, the elbow point is 11, so we'll pick 11 clusters.

After finding optimal K, we will use PCA to cluster the groups and find more information.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=13, fig.height=18}

pca_social = prcomp(X, rank=11, scale=TRUE)
loadings = pca_social$rotation
head(loadings)
summary(pca_social)

loadings_summary = loadings %>%
  as.data.frame() %>%
  rownames_to_column('Categories')
pc1_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC1), y=PC1)) + coord_flip()
pc2_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC2), y=PC2)) + coord_flip()
pc3_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC3), y=PC3)) + coord_flip()
pc4_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC4), y=PC4)) + coord_flip()
pc5_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC5), y=PC5)) + coord_flip()
pc6_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC6), y=PC6)) + coord_flip()
pc7_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC7), y=PC7)) + coord_flip()
pc8_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC8), y=PC8)) + coord_flip()
pc9_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC9), y=PC9)) + coord_flip()
pc10_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC10), y=PC10)) + coord_flip()
pc11_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC11), y=PC11))+ coord_flip()
grid.arrange(pc1_, pc2_, pc3_, pc4_, pc5_, pc6_, pc7_, pc8_, pc9_, pc10_, pc11_) 


```
As shown in the graphs above, people in different clusters have different interested in topics. Since it is a drinks brand, its target group should focus on the consumer who is interested in cooking or the health/fitness crowd. In this case, sending more advertisements to clusters 2 and 3 seems a good idea. Moreover, college students are the group that is more willing to try new drinks, so clusters 4 and 5 are also ideal groups to appeal to. 

By using PCA and analyzing these value, the firm can have a better understanding of customers' interests and specifies categories of customers in each market segment.


## Question 3

Initially, it seemed like the best option to include relatively high thresholds for both support and confidence. This approach seems to make sense because support can tell us what rules are worth exploring further. However, when using a minimum support threshold of .005 and confidence of .5 we didn't seem to get very show-stopping results. Simply put, we basically determined that people buy whole milk and "other vegetables" when they buy other items. Given the sheer popularity of milk and vegetables, this isn't a very compelling or interesting result. Max item length was set at 10, this is because people typically purchase a lot of items at once when grocery shopping and we didn't want to miss any potentially interesting combinations. 

  The confidence threshold was set at .5, which may seem high, but setting confidence higher was done to offset the "milk" factor and to truly extract surprising results. Because milk is such a popular item, many rules that involve milk and another item will have high confidence even if the lift isn't very high. 

  After the disappointing results using .005 minimum support, we adjusted our minimum support to be .001 while keeping confidence and max item length the same. After extracting the rules, we looked at rules with a lift > 10, and this resulted in some interesting, but not entirely surprising associations.\newpage
  
**The 15 rules with lift greater than 10 are listed below:** 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, dpi = 300)
library(tidyverse)
library(knitr)
library(reshape2)
library(foreach)
library(gridExtra)
library(mosaic)
library(arules) 
library(arulesViz)
library(igraph)
theme_set(theme_minimal())
```
```{r 3A, results = FALSE, message=FALSE, echo=FALSE}
#import data, tab delimited text file
raw_groceries <- read.table("groceries.txt", 
                            sep = '\t', header = F)
#add a numbered variable to keep track of baskets
raw_groceries <- raw_groceries %>% add_column(Basket_Number= NA) 
raw_groceries$Basket_Number <- 1:nrow(raw_groceries) 
#rename first variable
raw_groceries <- rename(raw_groceries, Customer = V1)
raw_groceries$Basket_Number = factor(raw_groceries$Basket_Number)
#commas into individual items
groceries = strsplit(raw_groceries$Customer, ",")
groceries$Customer = factor(groceries$Customer)
# Remove duplicates ("de-dupe")
# lapply says "apply a function to every element in a list"
# unique says "extract the unique elements" (i.e. remove duplicates)
groceries = lapply(groceries, unique)
grocerytrans = as(groceries, "transactions")
groceryrules = apriori(grocerytrans, 
                     parameter=list(support=.001, confidence=.5, maxlen=10))
grocrules_df <- arules::DATAFRAME(groceryrules)
data_frame_mod <- filter(grocrules_df,lift>10)
sub1 = subset(groceryrules, subset = confidence > 0.01 & support > 0.005)
```

```{r 3B, message=FALSE, echo=FALSE}
kable(data_frame_mod[1:15, ], caption = "Rules with lift over 10")
```

**Looking at many of the rules, it's clear that some are compliments such as:**
  
  `{ham, processed cheese} -> white bread`
  
  `{baking powder, flour} -> sugar`
  
  **Other rules might not initially seem like complements, but have clear associations with each other. The rule with the highest lift seems to come from people planning parties or cookouts:**
  
  `{instant food products, soda} -> hamburger meat` 
  
This rule has the highest lift of all the rules we found with 18.998 lift, and may indicate people buying products for cookouts.
  
  `{liquor, red/blush wine} -> bottled beer`
  
  This rule makes sense for parties, it also has a very high confidence of 0.9047619.
  
  `{popcorn, soda} -> salty snack` 
  
  This rule makes sense because people buy these items for parties and movie nights
  
  **Finally, the most amusing rule may be:**
  
  `{Instant food products, whole milk} -> hamburger meat` 
  
  This rule may be comprised of people buying the ingredients for the American household staple Hamburger Helper, which requires instant Hamburger Helper mix, milk, and hamburger meat.  
  
### Graphs

Below are some plots illustrating the ruleset created in the first part of the question.

Plot 1 shows rules organized by support and lift, with shade intensity representing confidence.

Plot 2 shows rules organized support and confidence with different colors representing the order of specific rules.
  
```{r 3C, message=FALSE, echo=FALSE}
plot(groceryrules, measure = c("support", "lift"), shading = "confidence")
# "two key" plot: coloring is by size (order) of item set
plot(groceryrules, method='two-key plot')
```




